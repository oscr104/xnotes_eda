{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief\n",
    "\n",
    "Scenario: You are part of a team tasked with developing findings on the spread of misinformation surrounding the US presidential elections on X. The team has been asked to develop work that explores the kinds of election misinformation narratives that are spreading on the platform. You have been asked to examine the dataset of Community Notes to find potential leads. Information on what the dataset contains is available here.\n",
    "\n",
    "## Task 1: \n",
    "Do some simple initial exploratory work on the dataset to help the team understand its contents and potential directions for research. Your findings should be presented as a one-page document that:\n",
    "1. Is understandable by a non-technical audience\n",
    "2. Contains basic statistics that could help direct further work\n",
    "\n",
    "You can use bullet points and visualizations if you wish. Please submit your notebook to show your working alongside the document. You should work in Python.  \n",
    "\n",
    "## Task 2: \n",
    "\n",
    "Create a one-page plan for a two-week analysis project on the dataset that uses more complex methods. You can assume that you would have help from colleagues with this task. As part of this task, you should suggest some expected top-line findings for the final research piece that your analysis could provide. Please also state the specific Python modules you would use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notes: Contains a table representing all notes\n",
    "- Ratings: Contains a table representing all ratings\n",
    "- Note Status History: Contains a table with metadata about notes including what statuses they received and when.\n",
    "- User Enrollment: Contains a table with metadata about each user's enrollment state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "import tools as tl\n",
    "import importlib\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data load & cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLS for dataset downloads\n",
    "\n",
    "note_url = (\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/notes/notes-00000.tsv\"\n",
    ")\n",
    "ratings_urls = [\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00005.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00004.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00000.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00006.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00002.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00001.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00007.tsv\",\n",
    "    \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteRatings/ratings-00003.tsv\",\n",
    "]\n",
    "status_history_url = \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/noteStatusHistory/noteStatusHistory-00000.tsv\"\n",
    "user_enrol_status_url = \"https://ton.twimg.com/birdwatch-public-data/2024/10/17/userEnrollment/userEnrollment-00000.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running for first time:\n",
    "# tl.download_to_parquet(note_url, fname=\"notes\")\n",
    "notes = pd.read_parquet(\"data/notes.parquet\")\n",
    "notes = notes.dropna(subset=\"summary\")\n",
    "notes[\"datetime\"] = pd.to_datetime(notes[\"createdAtMillis\"], unit=\"ms\")\n",
    "notes[\"harmful\"] = notes[\"harmful\"].fillna(\"Null\")\n",
    "notes[\"believable\"] = notes[\"believable\"].fillna(\"Null\")\n",
    "notes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment download line if running for first time\n",
    "for idx, url in enumerate(ratings_urls):\n",
    "    # tl.download_to_parquet(url, fname=f\"ratings_{idx}\")\n",
    "    df = pd.read_parquet(f\"data/ratings_{idx}.parquet\")\n",
    "    if idx == 0:\n",
    "        ratings = df\n",
    "    else:\n",
    "        ratings = pd.concat([ratings, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_parquet(f\"data/ratings_0.parquet\")\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl.download_to_parquet(status_history_url, fname=\"stat_hist\")\n",
    "stat_hist = pd.read_parquet(\"data/stat_hist.parquet\")\n",
    "stat_hist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl.download_to_parquet(user_enrol_status_url, fname=\"user_stat\")\n",
    "user_stat = pd.read_parquet(\"data/user_stat.parquet\")\n",
    "user_stat.rename(columns={\"participantId\": \"noteAuthorParticipantId\"}, inplace=True)\n",
    "user_stat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Schema of data tables\n",
    "\n",
    "Understand how information can be linked between these separate tables via matching columns (IE finding join keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = {\n",
    "    \"notes\": notes.columns,\n",
    "    \"user_stats\": user_stat.columns,\n",
    "    \"ratings\": ratings.columns,\n",
    "    \"status_history\": stat_hist.columns,\n",
    "}\n",
    "graph_colors = {\n",
    "    \"notes\": \"#ea5545\",\n",
    "    \"user_stats\": \"#ef9b20\",\n",
    "    \"ratings\": \"#87bc45\",\n",
    "    \"status_history\": \"#f46a9b\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "for table in all_cols.keys():\n",
    "    table_graph = nx.Graph()\n",
    "    table_graph.add_node(table, color=graph_colors[table])\n",
    "    for node in list(all_cols[table]):\n",
    "        table_graph.add_node(node, color=\"gainsboro\")\n",
    "        table_graph.add_edge(table, node)\n",
    "    g = nx.compose(g, table_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.plot_single_graph(g, layout=\"neato\", color_attr=True, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degree_dict = nx.degree(g)\n",
    "g2 = nx.subgraph(g, [x for x in g.nodes() if node_degree_dict[x] > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.plot_single_graph(g2, color_attr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables can be joined on noteId, participantId, and createdAtMillis for full data on a given author or note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Notes dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[\"summary_len\"] = notes[\"summary\"].apply(lambda x: len(x.split()))\n",
    "print(notes[\"summary_len\"].mean())\n",
    "print(notes[\"summary_len\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine whether there is any apparent relationship between the binary scores in the 'feature' columns (user tags on the characteristics of a post), and its harmfulness rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "feature_cols = [\n",
    "    \"misleadingOther\",\n",
    "    \"misleadingFactualError\",\n",
    "    \"misleadingManipulatedMedia\",\n",
    "    \"misleadingOutdatedInformation\",\n",
    "    \"misleadingMissingImportantContext\",\n",
    "    \"misleadingUnverifiedClaimAsFact\",\n",
    "    \"misleadingSatire\",\n",
    "    \"notMisleadingOther\",\n",
    "    \"notMisleadingFactuallyCorrect\",\n",
    "    \"notMisleadingOutdatedButNotWhenWritten\",\n",
    "    \"notMisleadingClearlySatire\",\n",
    "    \"notMisleadingPersonalOpinion\",\n",
    "    \"trustworthySources\",\n",
    "]\n",
    "\n",
    "\n",
    "notes_sum = (\n",
    "    notes.groupby([\"harmful\"])[feature_cols]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .melt(id_vars=[\"harmful\"], var_name=\"category\", value_name=\"count\")\n",
    ")\n",
    "\n",
    "notes_sum[\"proportion\"] = None\n",
    "for harm, reason in itertools.product(\n",
    "    notes_sum[\"harmful\"].unique(), notes_sum[\"category\"].unique()\n",
    "):\n",
    "    count = notes_sum[\n",
    "        (notes_sum[\"harmful\"] == harm) & (notes_sum[\"category\"] == reason)\n",
    "    ][\"count\"].values[0]\n",
    "    total_at_harm = notes_sum[(notes_sum[\"harmful\"] == harm)][\"count\"].sum()\n",
    "    notes_sum.loc[\n",
    "        (notes_sum[\"harmful\"] == harm) & (notes_sum[\"category\"] == reason), \"proportion\"\n",
    "    ] = (count / total_at_harm)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "colors = [\"firebrick\", \"khaki\", \"forestgreen\"]\n",
    "sns.barplot(\n",
    "    ax=ax, x=\"category\", y=\"proportion\", hue=\"harmful\", data=notes_sum, palette=colors\n",
    ")\n",
    "fig = plt.gcf()\n",
    "fig.autofmt_xdate()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All categories (considerable_harm, little_harm, and null) have modal features: misleadingFactualError, misleadingMissingImportantContext, misleadingUnverifiedClaimAsFact, and trustworthySources.\n",
    "- considerable_harm posts are more likely to be tagged with the 'misleading' modal features from this list, and less likely to have 'trustworthy sources'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Notes data over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate pre & post-takeover data\n",
    "\n",
    "notes_pre = notes[notes[\"datetime\"] < dt.strptime(\"2022-10-28\", \"%Y-%m-%d\")]\n",
    "notes_post = notes[notes[\"datetime\"] > dt.strptime(\"2022-10-28\", \"%Y-%m-%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    notes,\n",
    "    x=\"datetime\",\n",
    "    color_discrete_sequence=[\"firebrick\"],\n",
    "    opacity=0.8,\n",
    "    width=1600,\n",
    "    height=450,\n",
    ")\n",
    "fig.update_layout(\n",
    "    margin=dict(l=20, r=20, t=30, b=20),\n",
    "    xaxis_title=None\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_vline(\n",
    "    x=dt.strptime(\"2022-10-28\", \"%Y-%m-%d\").timestamp() * 1000,\n",
    "    annotation_text=\"Change in management\",\n",
    "    annotation=dict(font_size=18),\n",
    "    opacity=0.2,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.histogram(\n",
    "    notes_pre,\n",
    "\n",
    "    x=\"datetime\",\n",
    "    color=\"harmful\",\n",
    "    color_discrete_sequence=[\"firebrick\", \"khaki\", \"forestgreen\"],\n",
    "    title=\"Pre-takeover, colored by harm category\",\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.histogram(\n",
    "    notes_pre,\n",
    "\n",
    "    x=\"datetime\",\n",
    "    color=\"believable\",\n",
    "    color_discrete_sequence=[\"firebrick\", \"khaki\", \"forestgreen\"],\n",
    "    title=\"Pre-takeover, colored by believability category\",\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[\"classification\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6  - Classifying harmfulness level based on feature columns\n",
    " We can use the pre-takeover flags of 'considerable' and 'little' harm as labels to train an NN classifier, using the feature columns as our training data. This may allow us to estimate the proportions of post-takeover posts in the 'harmful' and 'little' harm categories, even though these labels were discontinued post-takeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train-test portion from prediction portion\n",
    "tt_notes = notes[notes[\"datetime\"] < dt.strptime(\"2022-10-28\", \"%Y-%m-%d\")]\n",
    "\n",
    "\n",
    "pred_notes = notes[notes[\"datetime\"] > dt.strptime(\"2022-10-28\", \"%Y-%m-%d\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split from pre-takeover data\n",
    "feature_cols = [\n",
    "    \"misleadingOther\",\n",
    "    \"misleadingFactualError\",\n",
    "    \"misleadingManipulatedMedia\",\n",
    "    \"misleadingOutdatedInformation\",\n",
    "    \"misleadingMissingImportantContext\",\n",
    "    \"misleadingUnverifiedClaimAsFact\",\n",
    "    \"misleadingSatire\",\n",
    "    \"notMisleadingOther\",\n",
    "    \"notMisleadingFactuallyCorrect\",\n",
    "    \"notMisleadingOutdatedButNotWhenWritten\",\n",
    "    \"notMisleadingClearlySatire\",\n",
    "    \"notMisleadingPersonalOpinion\",\n",
    "    \"trustworthySources\",\n",
    "]\n",
    "\n",
    "\n",
    "X = tt_notes[feature_cols]\n",
    "\n",
    "\n",
    "\n",
    "y = tt_notes[\"harmful\"]\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 100, 100),\n",
    "    max_iter=500,\n",
    "    alpha=0.0001,\n",
    "    solver=\"sgd\",\n",
    "    verbose=10,\n",
    "    random_state=21,\n",
    "    tol=0.000000001,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our overall performance (weighted avg) is 75% precision,  78% recall.\n",
    "\n",
    "This is decent, but not fantastic. Could potentially be improved with some feature engineering in the future. We probably wouldnt want to use this model to predict at the level of individual posts, but the performance is sufficient for us to examine the overall trend of harmful categorised posts after the takeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on post-takeover data\n",
    "post_data = notes_post[feature_cols]\n",
    "post_dts = notes_post[\"datetime\"]\n",
    "\n",
    "post_pred = clf.predict(post_data)\n",
    "pred_df = pd.DataFrame(columns=[\"harmful\"], data=post_pred)\n",
    "pred_df[\"datetime\"] = post_dts\n",
    "\n",
    "# Plot results\n",
    "\n",
    "color_dict = {\n",
    "    \"CONSIDERABLE_HARM\": \"firebrick\",\n",
    "    \"LITTLE_HARM\": \"forestgreen\",\n",
    "    \"Null\": \"khaki\",\n",
    "}\n",
    "\n",
    "\n",
    "pred_df = pred_df.sort_values(by=\"harmful\")\n",
    "\n",
    "fig = px.histogram(\n",
    "    pred_df,\n",
    "    x=\"datetime\",\n",
    "    color=\"harmful\",\n",
    "    color_discrete_map=color_dict,\n",
    "    opacity=0.8,\n",
    "    title=\"Predicted harm labels for post-takeover notes\",\n",
    "    width=900,\n",
    "    height=450,\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    y=0.99,\n",
    "    xanchor=\"left\",\n",
    "    x=0.01\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df[\"datetime\"] > dt.strptime(\"2024-01-01\", \"%Y-%m-%d\")][\n",
    "    \"harmful\"\n",
    "].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_notes[tt_notes[\"datetime\"] < dt.strptime(\"2024-01-01\", \"%Y-%m-%d\")][\n",
    "    \"harmful\"\n",
    "].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat test on harmfulness proportions, pre vs post takeover\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "f_obs = pred_df[\"harmful\"].value_counts(normalize=True).mul(100).values\n",
    "f_exp = tt_notes[\"harmful\"].value_counts(normalize=True).mul(100).values\n",
    "chisquare(f_obs=f_obs, f_exp=f_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the recent & unlabelled notes probably relate to \"considerable harmful\" content!\n",
    "\n",
    "Lets build a network graph of commonly occuring named entities in the predicted harmful CN summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map predicted harm labels onto post-takeover notes dataframe\n",
    "notes_post = notes_post.merge(pred_df[[\"datetime\", \"harmful\"]], on=\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_notes = notes_post[\n",
    "    (notes_post[\"harmful\"] == \"CONSIDERABLE_HARM\")\n",
    "    & (notes_post[\"summary\"].str.contains(\"election|voting|voter\"))\n",
    "].reset_index()\n",
    "g = tl.entity_graph(harmful_notes, top_prop=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc = max(nx.connected_components(g), key=len)  # get largest component of graph\n",
    "subg = g.subgraph(largest_cc).copy()\n",
    "tl.plot_single_graph(subg, layout=\"neato\", save_path=\"plots/election_network.png\", figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of familiar names, and the network structure makes intuitive sense (big names in the centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted((d for n, d in subg.degree()), reverse=True)\n",
    "dmax = max(degree_sequence)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "axs[0].plot(degree_sequence, \"b-\", marker=\"o\")\n",
    "axs[0].set_title(\"Degree Rank Plot\")\n",
    "axs[0].set_ylabel(\"Degree\")\n",
    "axs[0].set_xlabel(\"Rank\")\n",
    "\n",
    "\n",
    "axs[1].bar(*np.unique(degree_sequence, return_counts=True))\n",
    "axs[1].set_title(\"Degree histogram\")\n",
    "axs[1].set_xlabel(\"Degree\")\n",
    "axs[1].set_ylabel(\"# of Nodes\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest degree nodes:\n",
    "node_degrees = nx.degree_centrality(subg)\n",
    "sorted_degrees = dict(\n",
    "    sorted(node_degrees.items(), key=lambda item: item[1], reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique Authors:\")\n",
    "print(len(notes[\"noteAuthorParticipantId\"].unique()))\n",
    "\n",
    "print(\"Number of notes\")\n",
    "print(notes.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[\"noteAuthorParticipantId\"].value_counts(normalize=True).mul(100).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "notes[\"noteAuthorParticipantId\"].value_counts().plot.hist(\n",
    "    ax=axs[0], bins=50, log=True, alpha=0.8\n",
    ")\n",
    "axs[0].set_xlabel(\"note Author Participant Id counts in dataset\")\n",
    "axs[0].set_ylabel(\"Number\")\n",
    "axs[0].set_title(\"noteAuthor counts\")\n",
    "\n",
    "notes[\"noteAuthorParticipantId\"].value_counts(normalize=True).mul(100).head(\n",
    "    10\n",
    ").plot.bar(ax=axs[1], alpha=0.8)\n",
    "axs[1].set_xlabel(\"User\")\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_ylabel(\"% of total notes\")\n",
    "axs[1].set_title(\"Contribution of 10 most active users\")\n",
    "\n",
    "\n",
    "sns.despine()\n",
    "sns.set_context(\"notebook\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vast majority of unique Author IDs have very few entries in the dataframe. Very few users have >4000 entries in the dataframe, however those users are associated with a very disproportionate amount of the records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 User stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_from_value_counts(df, col, proportional=False, kind=\"bar\", date_fmt=False):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    counts = df[col].value_counts(normalize=proportional)\n",
    "    counts.plot(kind=kind, ax=axs[0], alpha=0.8)\n",
    "    axs[0].set_xlabel(col)\n",
    "    axs[0].set_ylabel(\"#\")\n",
    "\n",
    "    counts.plot(kind=kind, ax=axs[1], alpha=0.8, logy=True)\n",
    "    axs[1].set_xlabel(col)\n",
    "    axs[1].set_ylabel(\"# log scale\")\n",
    "    if date_fmt:\n",
    "        fig = plt.gcf()\n",
    "        fig.autofmt_xdate()\n",
    "    sns.despine()\n",
    "    sns.set_context(\"notebook\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = [\n",
    "    \"enrollmentState\",\n",
    "    \"successfulRatingNeededToEarnIn\",\n",
    "    \"modelingPopulation\",\n",
    "    \"modelingGroup\",\n",
    "    \"numberOfTimesEarnedOut\",\n",
    "]\n",
    "\n",
    "print(user_stat[\"enrollmentState\"].value_counts(normalize=True))\n",
    "plots_from_value_counts(user_stat, \"enrollmentState\", date_fmt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vast majority (~97%) of notes rating users are 'new' (58%) or 'earned in' (38%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_from_value_counts(user_stat, \"modelingPopulation\", date_fmt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of users are in the 'core' group, supposedly used as a reliable baseline of longer term contributors. Smaller proportion in the 'expansion' and 'expansion plus' populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for population in [\"CORE\", \"EXPANSION\", \"EXPANSION_PLUS\"]:\n",
    "    pop_stat = user_stat[user_stat[\"modelingPopulation\"] == population]\n",
    "    print(population)\n",
    "    print(pop_stat[\"enrollmentState\"].value_counts(normalize=True))\n",
    "    plots_from_value_counts(pop_stat, \"enrollmentState\", date_fmt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, population in enumerate([\"CORE\", \"EXPANSION\", \"EXPANSION_PLUS\"]):\n",
    "    pop_stat = user_stat[user_stat[\"modelingPopulation\"] == population]\n",
    "    percentages = pop_stat[\"enrollmentState\"].value_counts(normalize=True).mul(100).rename(population).reset_index()\n",
    "    if idx == 0:\n",
    "        df = percentages\n",
    "    else:\n",
    "        df = df.merge(percentages, on=\"enrollmentState\")\n",
    "df = df.set_index(\"enrollmentState\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "res = chi2_contingency(df.values, correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populations look fairly similar w.r.t. user privileges. Might be worth looking at their actual ratings/contributions to see if they differ or align vs the core population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_post[\"noteAuthorParticipantId\"] = notes_post[\"noteAuthorParticipantId\"].astype(\n",
    "    str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_post = notes_post.merge(\n",
    "    user_stat[[\"noteAuthorParticipantId\", \"modelingPopulation\"]],\n",
    "    on=\"noteAuthorParticipantId\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    notes_post,\n",
    "    x=\"datetime\",\n",
    "    color=\"modelingPopulation\",\n",
    "    color_discrete_sequence=[\"forestgreen\", \"khaki\", \"firebrick\"],\n",
    "    opacity=0.8,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC User status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_stat[\"numberOfTimesEarnedOut\"].value_counts(normalize=True))\n",
    "plots_from_value_counts(user_stat, \"numberOfTimesEarnedOut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_users = user_stat[user_stat[\"numberOfTimesEarnedOut\"] >= 10][\n",
    "    \"noteAuthorParticipantId\"\n",
    "].unique()\n",
    "\n",
    "len(problem_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stat[user_stat[\"noteAuthorParticipantId\"].isin(problem_users)][\"modelingPopulation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_user_notes = notes[\n",
    "    notes[\"noteAuthorParticipantId\"].isin(problem_users)\n",
    "].reset_index()\n",
    "\n",
    "problem_user_notes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 2939 notes authored by 3 'problem users' (users who have been 'earned out' 10 or more times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tl.entity_graph(problem_user_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cc = max(nx.connected_components(g), key=len)  # get largest component of graph\n",
    "subg = g.subgraph(largest_cc).copy()\n",
    "tl.plot_single_graph(subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[\"problem\"] = False\n",
    "notes.loc[notes[\"noteAuthorParticipantId\"].isin(problem_users), \"problem\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_notes = notes[notes[\"problem\"]==True]\n",
    "print(problem_notes[[\"noteId\", \"summary\"]].head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentially interesting - are these multiple-suspended users repeatedly posting bad info? or just unpopular takes? Could be an interesting thread, especially if theyre posting bad political/election content and not being permanently banned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC helpfulness ratings of notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "eature_cols = [\n",
    "    \"helpfulInformative\",\n",
    "    \"helpfulClear\",\n",
    "    \"helpfulEmpathetic\",\n",
    "    \"helpfulGoodSources\",\n",
    "    \"helpfulUniqueContext\",\n",
    "    \"helpfulAddressesClaim\",\n",
    "    \"helpfulImportantContext\",\n",
    "    \"helpfulUnbiasedLanguage\",\n",
    "    \"notHelpfulOther\",\n",
    "    \"notHelpfulIncorrect\",\n",
    "    \"notHelpfulSourcesMissingOrUnreliable\",\n",
    "    \"notHelpfulOpinionSpeculationOrBias\",\n",
    "    \"notHelpfulMissingKeyPoints\",\n",
    "    \"notHelpfulOutdated\",\n",
    "    \"notHelpfulHardToUnderstand\",\n",
    "    \"notHelpfulArgumentativeOrBiased\",\n",
    "    \"notHelpfulOffTopic\",\n",
    "    \"notHelpfulSpamHarassmentOrAbuse\",\n",
    "    \"notHelpfulIrrelevantSources\",\n",
    "    \"notHelpfulOpinionSpeculation\",\n",
    "    \"notHelpfulNoteNotNeeded\",\n",
    "]\n",
    "\n",
    "\n",
    "ratings_sum = (\n",
    "    ratings.groupby([\"helpfulnessLevel\"])[feature_cols]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .melt(id_vars=[\"helpfulnessLevel\"], var_name=\"category\", value_name=\"count\")\n",
    ")\n",
    "\n",
    "ratings_sum[\"proportion\"] = None\n",
    "for helpfulness, reason in itertools.product(\n",
    "    ratings_sum[\"helpfulnessLevel\"].unique(), ratings_sum[\"category\"].unique()\n",
    "):\n",
    "    count = ratings_sum[\n",
    "        (ratings_sum[\"helpfulnessLevel\"] == helpfulness)\n",
    "        & (ratings_sum[\"category\"] == reason)\n",
    "    ][\"count\"].values[0]\n",
    "    total_at_hfulness = ratings_sum[(ratings_sum[\"helpfulnessLevel\"] == helpfulness)][\n",
    "        \"count\"\n",
    "    ].sum()\n",
    "    ratings_sum.loc[\n",
    "        (ratings_sum[\"helpfulnessLevel\"] == helpfulness)\n",
    "        & (ratings_sum[\"category\"] == reason),\n",
    "        \"proportion\",\n",
    "    ] = (\n",
    "        count / total_at_hfulness\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "colors = [\"forestgreen\", \"firebrick\", \"khaki\"]\n",
    "sns.barplot(\n",
    "    ax=ax,\n",
    "    x=\"category\",\n",
    "    y=\"proportion\",\n",
    "    hue=\"helpfulnessLevel\",\n",
    "    data=ratings_sum,\n",
    "    palette=colors,\n",
    ")\n",
    "fig = plt.gcf()\n",
    "fig.autofmt_xdate()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC - Free text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(tl)\n",
    "focus_df = notes[(notes[\"datetime\"] > \"2024-01-01\")].reset_index()\n",
    "print(focus_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-stage cleanup/filter for english language content\n",
    "1. ASCII latin encoded only\n",
    "2. English language detected only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_df = focus_df.dropna(subset=[\"summary\"])\n",
    "focus_df[\"asci\"] = focus_df[\"summary\"].apply(lambda x: x.isascii())\n",
    "focus_df = focus_df[focus_df[\"asci\"] == True]\n",
    "focus_df.reset_index(inplace=True)\n",
    "print(focus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # 1#\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe(\"language_detector\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_check(text):\n",
    "    doc = nlp(text)\n",
    "    lang = doc._.language\n",
    "    if lang[\"score\"] > 0.6:\n",
    "        return True\n",
    "    else:\n",
    "        return\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for idx, row in tqdm(focus_df.iterrows(), total=focus_df.shape[0]):\n",
    "    text = row[\"summary\"]\n",
    "    doc = nlp(text)\n",
    "    lang = doc._.language\n",
    "    if lang[\"score\"] > 0.6:\n",
    "        focus_df.at[idx, \"language\"] = lang[\"language\"]\n",
    "\n",
    "\"\"\"\n",
    "tqdm.pandas()\n",
    "focus_df[\"eng\"] = focus_df[\"summary\"].progress_apply(eng_check)\n",
    "\n",
    "\n",
    "# save if first time running\n",
    "\n",
    "# notes.to_parquet(\"notes_fmt.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_df.to_parquet(\"data/focus_df.parquet\")\n",
    "# focus_df = pd.read_parquet(\"data/focus_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subselect to English only\n",
    "focus_df = focus_df[focus_df[\"eng\"] == True]\n",
    "focus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword subselection\n",
    "focus_df = focus_df[(focus_df[\"summary\"].str.contains(\"election|vote|voting\"))]\n",
    "focus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph network of entities in election posts, keeping top 50% of entities (by occurence)\n",
    "g = tl.entity_graph(focus_df, top_prop=0.5)\n",
    "largest_cc = max(nx.connected_components(g), key=len)  # get largest component of graph\n",
    "subg = g.subgraph(largest_cc).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml_lxml(subg, \"data/focus_g.graphml\")\n",
    "# subg = nx.read_graphml(\"data/focus_g.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.plot_single_graph(subg, layout=\"neato\", figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is still very big and needs some cleanup (e.g. duplicates, single name entities), but we can already see some structure here:\n",
    "- There are apparent clusters relating to political activity in several countries (e.g. US, UK, India)\n",
    "- 'hub' nodes for some important names (e.g. Narendra Modi, Boris Johnson, Trump/Harris/Biden/Musk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence = sorted((d for n, d in subg.degree()), reverse=True)\n",
    "dmax = max(degree_sequence)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "axs[0].plot(degree_sequence, \"b-\", marker=\"o\")\n",
    "axs[0].set_title(\"Degree Rank Plot\")\n",
    "axs[0].set_ylabel(\"Degree\")\n",
    "axs[0].set_xlabel(\"Rank\")\n",
    "\n",
    "\n",
    "axs[1].bar(*np.unique(degree_sequence, return_counts=True))\n",
    "axs[1].set_title(\"Degree histogram\")\n",
    "axs[1].set_xlabel(\"Degree\")\n",
    "axs[1].set_ylabel(\"# of Nodes\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest degree nodes:\n",
    "node_degrees = nx.degree_centrality(subg)\n",
    "sorted_degrees = dict(\n",
    "    sorted(node_degrees.items(), key=lambda item: item[1], reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_sequence[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_nodes = {k: sorted_degrees[k] for k in list(sorted_degrees)[:10]}\n",
    "hub_list = list(hub_nodes.keys())\n",
    "plt.bar(range(len(hub_nodes)), list(hub_nodes.values()), align=\"center\")\n",
    "plt.xticks(range(len(hub_nodes)), list(hub_nodes.keys()))\n",
    "plt.ylabel(\"Degree\")\n",
    "fig = plt.gcf()\n",
    "fig.autofmt_xdate()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_stats = stat_hist[stat_hist[\"currentStatus\"] != \"NEEDS_MORE_RATINGS\"]\n",
    "rated_stats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_stats[\"hr_to_first_stat\"] = (\n",
    "    rated_stats[\"timestampMillisOfFirstNonNMRStatus\"] - rated_stats[\"createdAtMillis\"]\n",
    ") / 3600000\n",
    "\n",
    "rated_stats[\"hr_to_current_stat\"] = (\n",
    "    rated_stats[\"timestampMillisOfCurrentStatus\"] - rated_stats[\"createdAtMillis\"]\n",
    ") / 3600000\n",
    "\n",
    "rated_stats[\"hr_between_first_and_current_stat\"] = (\n",
    "    rated_stats[\"timestampMillisOfCurrentStatus\"]\n",
    "    - rated_stats[\"timestampMillisOfFirstNonNMRStatus\"]\n",
    ") / 3600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_stats[\"days_to_current_stat\"] = rated_stats[\"hr_to_current_stat\"] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=rated_stats, x=\"days_to_current_stat\", hue=\"currentStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_stats.value_counts([\"firstNonNMRStatus\", \"currentStatus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
